{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client(\"sts\").get_caller_identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = os.getenv(\"SAGEMAKER_ROLE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "# try:\n",
    "#     role = sagemaker.get_execution_role()\n",
    "# except ValueError:\n",
    "iam = boto3.client(\"iam\")\n",
    "role = iam.get_role(RoleName=role_name)[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/processed/mistral/dolly/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "use_spot_instances = True\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = (\n",
    "    f'llama2-huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    ")\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id,  # pre-trained model\n",
    "    \"dataset_path\": \"/opt/ml/input/data/training\",  # path where sagemaker will save training dataset\n",
    "    \"epochs\": 3,  # number of training epochs\n",
    "    \"per_device_train_batch_size\": 2,  # batch size for training\n",
    "    \"lr\": 2e-4,  # learning rate used during training\n",
    "    \"hf_token\": HfFolder.get_token(),  # huggingface token to access llama 2\n",
    "    \"merge_weights\": True,  # wether to merge LoRA into the model (needs more memory) // we only have access to g5.2xlarge. This is not enough memory\n",
    "}\n",
    "\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"run_clm.py\",  # train script\n",
    "    source_dir=\"../scripts/sagemaker-llama2-qlora/\",  # directory which includes all the files needed for training\n",
    "    instance_type=\"ml.g5.2xlarge\",  # instances type used for the training job\n",
    "    instance_count=1,  # the number of instances used for training\n",
    "    base_job_name=job_name,  # the name of the training job\n",
    "    role=role,  # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size=300,  # the size of the EBS volume in GB\n",
    "    transformers_version=\"4.28\",  # the transformers version used in the training job\n",
    "    pytorch_version=\"2.0\",  # the pytorch_version version used in the training job\n",
    "    py_version=\"py310\",  # the python version used in the training job\n",
    "    hyperparameters=hyperparameters,  # the hyperparameters passed to the training job\n",
    "    # use_spot_instances   =  use_spot_instances, # wether to use spot instances or not\n",
    "    environment={\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\"\n",
    "    },  # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\"training\": training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
